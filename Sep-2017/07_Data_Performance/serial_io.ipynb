{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](../nci-logo.png)\n",
    "\n",
    "-------\n",
    "# Performance Examples of Using Data: Serial IO \n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "## In this notebook:\n",
    "\n",
    "<p>In this workshop we will explore the performance of accessing NetCDF4/HDF5 files by using NetCDF4-python and h5py modules. Several IO performance relating parameters, such as access pattern, storage layout, compression and shuffle etc. will be examined for serial IO.</p>\n",
    "\n",
    "The NetCDF architecture is shown below:\n",
    "\n",
    "<img src=\"images/netcdf_architecture.png\" alt=\"Drawing\" style=\"width: 600px;\">\n",
    "\n",
    "NetCDF4/HDF5 have many features like:\n",
    "- Unique technology suite that makes possible the management of extremely large and complex data collections\n",
    "- Unlimited size, extensibility, and portability\n",
    "- General data model\n",
    "- Unlimited variety of datatypes\n",
    "- Flexible, efficient I/O\n",
    "- Flexible data storage\n",
    "- Data transformation and complex subsetting\n",
    "\n",
    "\n",
    "#### The following material uses Geoscience Australia's Landsat 7 Data Collection which is available under the Create Commons License 4.0 through NCI's THREDDS Data Server. For more information on the collection and licensing, please [click here](http://geonetwork.nci.org.au/geonetwork/srv/eng/catalog.search#/metadata/f6600_8228_7170_1486). \n",
    "\n",
    "---------\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "#### Please load the following modules:\n",
    "\n",
    "```\n",
    "module load python/2.7.11\n",
    "module load python/2.7.11-matplotlib\n",
    "module load ipython/4.2.0-py2.7\n",
    "module load netcdf4-python/1.2.4-py2.7\n",
    "\n",
    "```\n",
    "\n",
    "#### Then launch a new iPython (or Jupyter) notebook: \n",
    "\n",
    "`$ ipython notebook`\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from tempfile import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "global tmp_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function view_prt : View the metadata of a variable named \"varname\" of a file named \"filename\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_prt(filename,varname='all'):\n",
    "    f=Dataset(filename,'r')                      # Open the file named 'filename' for read\n",
    "    print 'File Name: \\t',filename               # Print the file name \n",
    "    print 'Format: \\t',f.data_model              # Print the file format\n",
    "\n",
    "    for item in f.dimensions:                    # Print all dimensions\n",
    "        print 'dimension \\t',f.dimensions[item].name, f.dimensions[item].size\n",
    "    print '    '\n",
    "\n",
    "    vars = f.variables.keys()\n",
    "    for item in vars:\n",
    "        if varname=='all' or item==varname:      # Print the selected metadata of all \n",
    "                                                 # variables or the variable named 'varname'\n",
    "            print 'Variable: \\t', item\n",
    "            print 'Dimensions: \\t', f[item].dimensions\n",
    "            print 'Shape:    \\t', f[item].shape\n",
    "            print \"size:  \\t\\t\", f[item].size\n",
    "            print \"data type:  \\t\", f[item].dtype\n",
    "            print \"chunksize: \\t\", f[item].chunking()\n",
    "            print \"tendian: \\t\", f[item].endian()\n",
    "            print \"filters: \\t\", f[item].filters()\n",
    "            print \"ncattrs: \\t\", f[item].ncattrs() \n",
    "            print \"\"\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function read_var: Read and return data of a NetCDF4 variable named \"varname\" in a file named \"filename\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_var(filename,varname):\n",
    "    fr = Dataset(filename,'r',format='NETCDF4')\n",
    "    out=fr[varname][:]\n",
    "    fr.close()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function link_var: Link a variable named \"varname\" in a file named \"filename\". No actual data movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def link_var(filename,varname):\n",
    "    fr = Dataset(filename,'r',format='NETCDF4')\n",
    "    return fr[varname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function create_file: Create a NetCDF4 files with 3 dimensions and a 3D variable named \"varname\" containing the data of 'val' array.\n",
    "Dimensions are used to define the shape of data in netCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(d1,d1_name,d2,d2_name,d3,d3_name,val,varname,**args):\n",
    "    global tmp_file \n",
    "    \n",
    "    try:  # remove any existing temporary files.\n",
    "        if os.path.exists(tmp_file.name):\n",
    "            tmp_file.close()\n",
    "            os.unlink(tmp_file.name)\n",
    "    except:\n",
    "        print \" \"\n",
    "\n",
    "    tmp_file = NamedTemporaryFile(delete=False) # Create the temporary file object.\n",
    "    fname=tmp_file.name                         # Get the temporary file name. \n",
    "\n",
    "    fw = Dataset(fname,'w',format='NETCDF4')    # Open the NetCDF file to write\n",
    "    \n",
    "    fw.createDimension(d1_name,len(d1))                 # Create the one-dimension named 'd1_name' \n",
    "                                                        # in the length of 'd1' array.\n",
    "    dim_wrt=fw.createVariable(d1_name,d1.dtype,d1_name) # Create a 1D variable named 'd1_name' \n",
    "                                                        # with the data type of array 'd1' \n",
    "                                                        # and its dimension is 'd1_name'\n",
    "    dim_wrt[:]=d1                                       # Write 'd1' data to dimension 'd1_name'\n",
    "\n",
    "    \n",
    "    \n",
    "    fw.createDimension(d2_name,len(d2))                 # Create the one-dimension named 'd2_name'\n",
    "                                                        # in the length of 'd2' array.\n",
    "    dim_wrt=fw.createVariable(d2_name,d2.dtype,d2_name) # Create one-dimension variable named 'd2_name' \n",
    "                                                        # with the data type of array 'd2'\n",
    "                                                        # and its dimension is 'd2_name'\n",
    "    dim_wrt[:]=d2                                       # Write the 'd2' data to dimension 'd2_name'\n",
    "\n",
    "\n",
    "    fw.createDimension(d3_name,len(d3))                 # Create the one-dimension named 'd3_name'\n",
    "                                                        # in the length of 'd3' array.\n",
    "    dim_wrt=fw.createVariable(d3_name,d3.dtype,d3_name) # Create one-dimension variable named 'd3_name' \n",
    "                                                        # with the data type of array 'd3'\n",
    "                                                        # and its dimension is 'd3_name'\n",
    "    dim_wrt[:]=d3                                       # Write the 'd3' data to dimension 'd3_name'\n",
    "    \n",
    "\n",
    "    var_wrt=fw.createVariable(\n",
    "        varname,val.dtype,(d1_name,d2_name,d3_name),**args) # Create one-dimension variable named 'varname' \n",
    "                                                            # with the data type of array 'val' along\n",
    "                                                            # 3 dimensions of 'd1_name, d2_name, d3_name'\n",
    "    var_wrt[:]=val                                          # write the 'val' data to the variable 'varname'\n",
    "    \n",
    "    fw.close()                       # Close the file\n",
    "    return tmp_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce example data source of t, x, y and data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_src(nt=100,ny=100,nx=100):\n",
    "    t = np.random.uniform(-1, 1, size=nt)\n",
    "    y = np.random.uniform(-1, 1, size=ny)\n",
    "    x= np.random.uniform(-1, 1, size=nx)\n",
    "    data=np.random.uniform(-1, 1, size=nt*ny*nx)\n",
    "    return t,y,x,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote vs. Local Access\n",
    "\n",
    "For local files, this will be the filepath (i.e., /g/data...) while for remote access, this will be the OPeNDAP data URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filename='http://dapds00.nci.org.au/thredds/dodsC/uc0/rs0_dev/multiple_band_variables/LS7_ETM_NBAR_P54_GANBAR01-002_089_078_2015_152_-26.nc'\n",
    "varname='band1'\n",
    "band=read_var(filename,varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filename='/g/data2/uc0/rs0_dev/multiple_band_variables/LS7_ETM_NBAR_P54_GANBAR01-002_089_078_2015_152_-26.nc'\n",
    "vaname='band1'\n",
    "band=read_var(filename,varname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create example 3D array in the size of (3600,60,60) along (t,y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,y,x,src_data=get_data_src(nt=3600,ny=60,nx=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Patterns\n",
    "We can access the dataset in arbitary manner such as time series access, spatial access and block access. The IO performance relies on the number of IO operataions, which is arised from the coordination beteween the access pattern and the storage layout of the file. The 'locality' rule of access is that reads are generally faster when the data being accessed is all stored together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fig1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce a variety of accesses from the time series one to the spatial one via different block accesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blk_size=[{'name':'spatial_st','t':     1,'y':len(y),'x':len(x)},\n",
    "          {'name':'block_t002','t':     2,'y':    30,'x':len(x)},\n",
    "          {'name':'block_t003','t':     3,'y':    20,'x':len(x)},\n",
    "          {'name':'block_t006','t':     6,'y':    10,'x':len(x)},\n",
    "          {'name':'block_t012','t':    12,'y':    10,'x':    30},\n",
    "          {'name':'block_t024','t':    24,'y':     5,'x':    30},  \n",
    "          {'name':'block_t120','t':   120,'y':     1,'x':    30},\n",
    "          {'name':'block_t180','t':   180,'y':     1,'x':    20},\n",
    "          {'name':'block_t360','t':   360,'y':     1,'x':    10},\n",
    "          {'name':'block_t600','t':   600,'y':     1,'x':     6},\n",
    "          {'name':'block_t900','t':   900,'y':     1,'x':     4},\n",
    "          {'name':'block_t1k2','t':  1200,'y':     1,'x':     3},\n",
    "          {'name':'block_t1k8','t':  1800,'y':     1,'x':     2},\n",
    "          {'name':'full_times','t':len(t),'y':     1,'x':     1}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index-order storage layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The default storage layout for most file format is index-order layout, i.e. all the elements of multi-dimensional array are stored one after another along each dimension. Python and C use row-major ordering,that is, the 'fastest-varying' index is the last dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tutr-lodset.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 3D array, the index-order data layout will store the data along x, y and t dimension respectively. Thus the 'spatial access' as above will be contiguous IO and the 'time series accesss' is non-contiguous IO with much more IO opeartions like seek, read, write etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYX: Create index order storage layout TYX in a temporary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a file with the 3D variable named 'band'\n",
    "f=create_file(t,'t',y,'y',x,'x',src_data,'band')\n",
    "view_prt(f.name,'band')\n",
    "# Print the metadata of 'band' variable in the above file.\n",
    "band=link_var(f.name,'band')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYX: Full variable access if we have large enough memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_start = time.clock()\n",
    "band_subset = band[:,:,:]\n",
    "t_end = time.clock() \n",
    "print \"elapsed time is \",t_end-t_start,\"Sec.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYX: Check the read time of different accesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into several 'read' operations and each operation reads 'transfer size' of data to match the memory requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elap_ctg_tyx=[]\n",
    "for item in blk_size:\n",
    "# Count the total number of accessing.\n",
    "    access_num=0\n",
    "    t_size=item['t']\n",
    "    y_size=item['y']\n",
    "    x_size=item['x']\n",
    "# Read the whole variable data in multiple read operations.\n",
    "    t_start = time.clock() \n",
    "    for it in range(len(t)/t_size):\n",
    "        itbeg=it*t_size\n",
    "        for iy in range(len(y)/y_size):\n",
    "            iybeg=iy*y_size\n",
    "            for ix in range(len(x)/x_size):\n",
    "                ixbeg=ix*x_size\n",
    "                band_subset = band[itbeg:itbeg+t_size,iybeg:iybeg+y_size,ixbeg:ixbeg+x_size]\n",
    "                access_num+=1\n",
    "    t_end = time.clock()\n",
    "    print item['name'], \" elapsed time is \",(t_end-t_start),\"Sec.\", \" access time is\",access_num\n",
    "    elap_ctg_tyx.append(t_end-t_start)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> For non-contiguous IO, number of disk accesses that make I/O slow, not the number of values read. </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YXT: Create index order storage layout YXT in a temporary file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> If we instead want the time series to be quick, we can reorganize the data so x or y is the most slowly varying dimension and time varies fastest, resulting in fast time-series access and slow spatial access. In either case, the slow access is so slow that it makes the data essentially inaccessible for all practical purposes, e.g. in analysis or visualization. <p>\n",
    "\n",
    "NCO (netCDF operators) program ncpdq (\"permute dimensions quickly\") can be used to exchange dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the variable along (y,x,t)\n",
    "f=create_file(y,'y',x,'x',t,'t',src_data,'band')\n",
    "view_prt(f.name,'band')\n",
    "band=link_var(f.name,'band')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YXT: Full Access if we have large enough memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "t_start = time.clock() \n",
    "band_subset = band[:,:,:]\n",
    "t_end = time.clock() \n",
    "print \"elapsed time is \",(t_end-t_start),\"Sec.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YXT: Check the read time of different accesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elap_ctg_yxt=[]\n",
    "for item in blk_size:\n",
    "# Count the total number of accessing.\n",
    "    access_num=0\n",
    "    t_size=item['t']\n",
    "    y_size=item['y']\n",
    "    x_size=item['x']\n",
    "# Read the whole variable data in multiple read operations.\n",
    "    t_start = time.clock() \n",
    "    for iy in range(len(y)/y_size):\n",
    "        iybeg=iy*y_size\n",
    "        for ix in range(len(x)/x_size):\n",
    "            ixbeg=ix*x_size\n",
    "            for it in range(len(t)/t_size):\n",
    "                itbeg=it*t_size\n",
    "                band_subset = band[iybeg:iybeg+y_size,ixbeg:ixbeg+x_size,itbeg:itbeg+t_size]\n",
    "                access_num+=1\n",
    "    t_end = time.clock()\n",
    "    print item['name'], \" elapsed time is \",(t_end-t_start),\"Sec.\", \" access time is\",access_num    \n",
    "    elap_ctg_yxt.append(t_end-t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunked storage layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> A better solution is the use of chunking, storing multidimensional data in multi-dimensional rectangular chunks to speed up slow accesses at the cost of slowing down fast accesses. Programs that access chunked data can be oblivious to whether or how chunking is used. <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/chunking.png\" alt=\"Drawing\" style=\"width: 600px;\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider these selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/contiguous_seek.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<p>If index-order: 2 seeks</p>\n",
    "If chunked: 10 seeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/chunk_seek.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<p>if index-order: 16 seeks</p>\n",
    "if chunked: 4 seeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> For 2-dimensional data, we could support equally frequent access by either rows or columns by chunking the data into rectangular chunks (or tiles) so that reading a row requires the same number of disk accesses as reading a column. \n",
    "Note each chunk is a disk block that must be read completely to access any of its data values. An optimum solution is to make the chunks similar in shape to the entire array, so that the same number of chunks are required to read an entire row or an entire column. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appropriate size of chunking depends on access patterns and hardware itself, like disk cache sizes, etc. When writing or reading, try to use hyperslab selections that coincide with chunk boundaries.\n",
    "\n",
    "The process of picking a chunk shape is a trade-off between the following constraints:\n",
    "\n",
    "<ul>\n",
    "<li> Larger chunks for a given dataset size reduce the size of the chunk B-tree, making it faster to find and load chunks. </li>\n",
    "<li> Since chunks are all or nothing (reading a portion loads the entire chunk), larger chunks also increase the chance that youâll read data into memory you wonât use. </li>\n",
    "<li> Chunk cache can only hold a finite number of chunks. Large chunk can not fit into the chunk cache. Entire chunk has to be in memory and may cause OS to page memory to disk, slowing down the entire system. </li>\n",
    "<li> Small chunks may create large amount of metadata to fill the disk space and extra time will be spent to look up each chunk. More IO operations will be invloved as each chunk is stored independently. A good rule of thumb for most datasets is to keep chunks above 10KiB or so. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>When writing or reading, try to use hyperslab selections that coincide with chunk boundaries.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYX: Chunk number=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the transfer size along each dimension\n",
    "chunk_number=20\n",
    "\n",
    "# Create the chunk shape. Make the number of chunks for z\n",
    "chunk_size = (len(t)/(chunk_number*chunk_number),len(y)/chunk_number,len(x)/chunk_number)\n",
    "\n",
    "# Create a chunked NetCDF4 file\n",
    "f=create_file(t,'t',y,'y',x,'x',src_data,'band',chunksizes=chunk_size)\n",
    "\n",
    "view_prt(f.name,'band')\n",
    "band=link_var(f.name,'band')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elap_ck20_tyx=[]\n",
    "for item in blk_size:\n",
    "# Count the total number of accessing.\n",
    "    access_num=0\n",
    "    t_size=item['t']\n",
    "    y_size=item['y']\n",
    "    x_size=item['x']\n",
    "# Read the whole variable data in multiple read operations.\n",
    "    t_start = time.clock() \n",
    "    for it in range(len(t)/t_size):\n",
    "        itbeg=it*t_size\n",
    "        for iy in range(len(y)/y_size):\n",
    "            iybeg=iy*y_size\n",
    "            for ix in range(len(x)/x_size):\n",
    "                ixbeg=ix*x_size\n",
    "                band_subset = band[itbeg:itbeg+t_size,iybeg:iybeg+y_size,ixbeg:ixbeg+x_size]\n",
    "                access_num+=1\n",
    "    t_end = time.clock()\n",
    "    print item['name'], \" elapsed time is \",(t_end-t_start),\"Sec.\", \" access time is\",access_num\n",
    "    elap_ck20_tyx.append(t_end-t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYX: Chunk number=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the transfer size along each dimension\n",
    "chunk_number=10\n",
    "# Create the chunk shape. Make the number of chunks for z\n",
    "chunk_size = (len(t)/(chunk_number*chunk_number),len(y)/chunk_number,len(x)/chunk_number)\n",
    "# Create a chunked NetCDF4 file\n",
    "f=create_file(t,'t',y,'y',x,'x',src_data,'band',chunksizes=chunk_size)\n",
    "\n",
    "view_prt(f.name,'band')\n",
    "band=link_var(f.name,'band')\n",
    "\n",
    "elap_ck10_tyx=[]\n",
    "for item in blk_size:\n",
    "# Count the total number of accessing.\n",
    "    access_num=0\n",
    "    t_size=item['t']\n",
    "    y_size=item['y']\n",
    "    x_size=item['x']\n",
    "# Read the whole variable data in multiple read operations.\n",
    "    t_start = time.clock() \n",
    "    for it in range(len(t)/t_size):\n",
    "        itbeg=it*t_size\n",
    "        for iy in range(len(y)/y_size):\n",
    "            iybeg=iy*y_size\n",
    "            for ix in range(len(x)/x_size):\n",
    "                ixbeg=ix*x_size\n",
    "                band_subset = band[itbeg:itbeg+t_size,iybeg:iybeg+y_size,ixbeg:ixbeg+x_size]\n",
    "                access_num+=1\n",
    "    t_end = time.clock()\n",
    "    print item['name'], \" elapsed time is \",(t_end-t_start),\"Sec.\", \" access time is\",access_num\n",
    "    elap_ck10_tyx.append(t_end-t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYX: Block access on matched chunk layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tblk=2\n",
    "yblk=30\n",
    "xblk=60\n",
    "chunk_size = [tblk,yblk,xblk]\n",
    "f=create_file(t,'t',y,'y',x,'x',src_data,'band',chunksizes=chunk_size)\n",
    "band=link_var(f.name,'band')\n",
    "t_start = time.clock() \n",
    "for it in range(len(t)/tblk):\n",
    "    itbeg=it*tblk\n",
    "    for iy in range(len(y)/yblk):\n",
    "        iybeg=iy*yblk\n",
    "        for ix in range(len(x)/xblk):\n",
    "            ixbeg=ix*xblk\n",
    "#            print itbeg,iybeg,ixbeg\n",
    "            band_subset = band[itbeg:itbeg+tblk-1,iybeg:iybeg+yblk-1,ixbeg:ixbeg+xblk-1]\n",
    "t_end = time.clock() \n",
    "print \"elapsed time is \",(t_end-t_start),\"Sec.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "data=[elap_ctg_tyx,elap_ctg_yxt,elap_ck20_tyx,elap_ck10_tyx]\n",
    "labels=['contiguous_tyx','contiguous_yxt','chunk20_tyx','chunk10_tyx']\n",
    "\n",
    "fig,axes=plt.subplots(nrows=1,ncols=1)\n",
    "bplot=axes.boxplot(data,vert=True,patch_artist=True)\n",
    "colors = ['pink', 'lightblue', 'lightgreen','lightyellow']\n",
    "for patch, color in zip(bplot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "plt.grid(True)   \n",
    "plt.ylabel('Read Time (s)')\n",
    "axes.set_yscale(\"log\")\n",
    "plt.setp(axes, xticks=[y+1 for y in range(len(data))],\n",
    "         xticklabels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If the data is compressed in netCDF-4 or HDF5, it has to be chunked, because a chunk is the atomic unit of compression as well as disk access. There is no need to decompress the whole file when reading a block data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunk scale = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,y,x,band2=get_data_src(nt=400,ny=100,nx=100)\n",
    "chunk_scale=20\n",
    "chunk_size = (len(t)/(chunk_scale*chunk_scale),len(y)/chunk_scale,len(x)/chunk_scale)\n",
    "print 'chunk size is ',chunk_size\n",
    "cmp_time_cs20=[]\n",
    "cmp_size_cs20=[]\n",
    "cmp_label_cs20=[]\n",
    "print '{:>20}{:>20}{:>20}'.format('Comp. Level','Write Time(s)', 'File Size(MB)')\n",
    "for ilevel in range(0,10):\n",
    "    start = time.clock() \n",
    "    f=create_file(t,'t',y,'y',x,'x',band2,'band2',chunksizes=chunk_size,complevel=ilevel,zlib=True)\n",
    "    elapsed = time.clock()\n",
    "    file_time = elapsed - start\n",
    "    file_size = os.path.getsize(f.name)/1048576.0\n",
    "    cmp_time_cs20.append(file_time)\n",
    "    cmp_size_cs20.append(file_size)\n",
    "    cmp_label_cs20.append(str(ilevel))\n",
    "    print '{:>20d}{:>20.2f}{:>20.2f}'.format(ilevel,file_time,file_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunk scale = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_scale=10\n",
    "chunk_size = (len(t)/(chunk_scale*chunk_scale),len(y)/chunk_scale,len(x)/chunk_scale)\n",
    "print 'chunk size is ',chunk_size\n",
    "\n",
    "cmp_time_cs10=[]\n",
    "cmp_size_cs10=[]\n",
    "cmp_label_cs10=[]\n",
    "\n",
    "print '{:>20}{:>20}{:>20}'.format('Comp. Level','Write Time(s)', 'File Size(MB)')\n",
    "for ilevel in range(0,10):\n",
    "    start = time.clock() \n",
    "    f=create_file(t,'t',y,'y',x,'x',band2,'band2',chunksizes=chunk_size,complevel=ilevel,zlib=True)\n",
    "    elapsed = time.clock()\n",
    "    file_time = elapsed - start\n",
    "    file_size = os.path.getsize(f.name)/1048576.0\n",
    "    cmp_time_cs10.append(file_time)\n",
    "    cmp_size_cs10.append(file_size)\n",
    "    cmp_label_cs10.append(str(ilevel))\n",
    "    print '{:>20d}{:>20.2f}{:>20.2f}'.format(ilevel,file_time,file_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunk scale = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_scale=5\n",
    "chunk_size = (len(t)/(chunk_scale*chunk_scale),len(y)/chunk_scale,len(x)/chunk_scale)\n",
    "print 'chunk size is ',chunk_size\n",
    "cmp_time_cs05=[]\n",
    "cmp_size_cs05=[]\n",
    "cmp_label_cs05=[]\n",
    "print '{:>20}{:>20}{:>20}'.format('Comp. Level','Write Time(s)', 'File Size(MB)')\n",
    "for ilevel in range(0,10):\n",
    "    start = time.clock() \n",
    "    f=create_file(t,'t',y,'y',x,'x',band2,'band2',chunksizes=chunk_size,complevel=ilevel,zlib=True)\n",
    "    elapsed = time.clock()\n",
    "    file_time = elapsed - start\n",
    "    file_size = os.path.getsize(f.name)/1048576.0\n",
    "    cmp_time_cs05.append(file_time)\n",
    "    cmp_size_cs05.append(file_size)\n",
    "    cmp_label_cs05.append(str(ilevel))\n",
    "    print '{:>20d}{:>20.2f}{:>20.2f}'.format(ilevel,file_time,file_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cmp_time_cs20, cmp_size_cs20,'.-',label='chunk scale=20')\n",
    "ax.plot(cmp_time_cs10, cmp_size_cs10,'.-',label=\"chunk scale=10\")\n",
    "ax.plot(cmp_time_cs05, cmp_size_cs05,'.-',label=\"chunk scale= 5\")\n",
    "\n",
    "plt.title('Compression Benchmark for 3D Random Data')\n",
    "ax.set_ylabel('File Size (MiB)')\n",
    "ax.set_xlabel('Compression Time (s)')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Landsat compression Benchmak is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/compress_time.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_prt(f.name,'band2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SHUFFLE filter repacks the data in the chunk so that all the first bytes of the integers\n",
    "are together, then all the second bytes, and so on. For dictionary-based compressors like GZIP and LZF, it is much more efficient to compress\n",
    "long runs of identical values, like all the zero values collected from the first two\n",
    "bytes of the dataset integers. There are also savings from the repeated elements at the\n",
    "third byte position. Only the fourth byte position looks really hard to compress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_scale=10\n",
    "chunk_size = (len(t)/(chunk_scale*chunk_scale),len(y)/chunk_scale,len(x)/chunk_scale)\n",
    "print 'chunk size is ',chunk_size\n",
    "\n",
    "cmp_time_nosuf=[]\n",
    "cmp_size_nosuf=[]\n",
    "cmp_label_nosuf=[]\n",
    "\n",
    "print '{:>20}{:>20}{:>20}'.format('Comp. Level','Write Time(s)', 'File Size(MB)')\n",
    "for ilevel in range(0,10):\n",
    "    start = time.clock() \n",
    "    f=create_file(t,'t',y,'y',x,'x',band2,'band2',chunksizes=chunk_size,complevel=ilevel,zlib=True,shuffle=False)\n",
    "    elapsed = time.clock()\n",
    "    file_time = elapsed - start\n",
    "    file_size = os.path.getsize(f.name)/1048576.0\n",
    "    cmp_time_nosuf.append(file_time)\n",
    "    cmp_size_nosuf.append(file_size)\n",
    "    cmp_label_nosuf.append(str(ilevel))\n",
    "    print '{:>20d}{:>20.2f}{:>20.2f}'.format(ilevel,file_time,file_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_prt(f.name,'band2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cmp_time_cs10, cmp_size_cs10,'.-',label='shuffle')\n",
    "ax.plot(cmp_time_nosuf, cmp_size_nosuf,'.-',label='no-shuffle')\n",
    "plt.title('Compression Benchmark for 3D Random Data')\n",
    "plt.legend(loc='best')\n",
    "ax.set_ylabel('File Size (MiB)')\n",
    "ax.set_xlabel('Compression Time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<ul>\n",
    "<li> 'Parallel HDF5',Quincey Koziol, The HDF group, 2015.</li>\n",
    "<li> 'Portable Parallel I/O: Handling large datasets in heterogeneous parallel environments', Michael Stephan, JULICH, 2013.</li>\n",
    "<li> 'Python and HDF5',Andrew Collette, 2014.</li>\n",
    "<li> 'Chunking Data: Why it Matters', http://www.unidata.ucar.edu/blogs/developer/entry/chunking_data_why_it_matters, 2013 </li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
