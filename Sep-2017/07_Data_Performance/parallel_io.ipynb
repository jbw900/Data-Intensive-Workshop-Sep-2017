{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../nci-logo.png)\n",
    "\n",
    "-------\n",
    "# Data Performance: Parallel IO\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### In this notebook:\n",
    "\n",
    "In this workshop we will demonstrate how to conduct the parallel IO in python by using multiprocessing module and MPI.\n",
    "\n",
    "There are three ways to do concurrent programming in Python: threads, the multiprocessing module, and the Message\n",
    "Passing Interface (MPI).\n",
    "\n",
    "Python itself includes a single master lock that governs access to the\n",
    "interpreterâs functions, called the Global Interpreter Lock or GIL. This lock serializes\n",
    "access from multiple threads to basic resources like object reference counting. We can\n",
    "have as many threads as we like in a Python program, but only one at a time can use\n",
    "the interpreter. Thus we canât use more than one coreâs worth of time when running a pure-Python program. \n",
    "Thread-based code is fine for GUIs and applications that call into external libraries that\n",
    "don't tie up the Python interpreter.\n",
    "<img src=\"images/threads.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "\n",
    "Multiprocessing provides support for basic fork()-based parallel processing. Itâs the simplest way in Python to write parallel code that uses more than one core, and strongly recommended for general-purpose computation.However, the parallel processes canât share a single NetCDF4/HDF5 file, even if the file is opened read only.  Alternatively, multiprocessing could be used to open multiple files respectively or doing computation in parallel and doing IO in serial.\n",
    "<img src=\"images/multiple_process.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "\n",
    "MPI-based Parallel HDF5 is by far the best way to do parallel IO on NetCDF4/HDF5 files. MPI is the official flavor of parallelism supported by the HDF5 library. You can have an unlimited number of processes, all of which share the same open HDF5 file. All of them can read and write data, and modify the fileâs structure. Programs written this way require a little more care,  but it is the most elegant and highest-performance way to use HDF5 in a parallel context.\n",
    "<img src=\"images/mpi_hdf5.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "\n",
    "\n",
    "#### The following material uses CSIRO IMOS/TERN MODIS Data Collection which is available under the Create Commons License 4.0 through NCI's THREDDS Data Server. For more information on the collection and licensing, please [click here](http://geonetwork.nci.org.au/geonetwork/srv/eng/catalog.search#/metadata/f3633_7369_2730_8213). \n",
    "\n",
    "---------\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "#### Please load the following modules:\n",
    "\n",
    "```\n",
    "module load python/2.7.11\n",
    "module load python/2.7.11-matplotlib\n",
    "module load ipython/4.2.0-py2.7\n",
    "module load netcdf4-python/1.2.4-py2.7\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "module load h5py/2.6.0-hdf5-1.8.14p-py2.7\n",
    "```\n",
    "\n",
    "\n",
    "#### Then launch a new Jupyter (iPython) notebook: \n",
    "\n",
    "`$ ipython notebook`\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple processes read multiple files\n",
    "\n",
    "Order matters here, netCDF4 needs to be loaded following the h5py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import commands\n",
    "from multiprocessing import Pool,current_process\n",
    "import numpy as np\n",
    "from tempfile import *\n",
    "from mpi4py import MPI\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import h5py\n",
    "global tmp_file\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the metadata of a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_prt(filename,varname='all'):\n",
    "    f=Dataset(filename,'r')\n",
    "    print 'File Name: \\t',filename\n",
    "    print 'Format: \\t',f.data_model\n",
    "    for item in f.dimensions:\n",
    "        print 'dimension \\t',f.dimensions[item].name, f.dimensions[item].size\n",
    "    print '    '\n",
    "    vars = f.variables.keys()\n",
    "    for item in vars:\n",
    "        if varname=='all' or item==varname:\n",
    "            print 'Variable: \\t', item\n",
    "            print 'Dimensions: \\t', f[item].dimensions\n",
    "            print 'Shape:    \\t', f[item].shape\n",
    "            print \"size:  \\t\\t\", f[item].size\n",
    "            print \"data type:  \\t\", f[item].dtype\n",
    "            print \"chunksize: \\t\", f[item].chunking()\n",
    "            print \"tendian: \\t\", f[item].endian()\n",
    "            print \"filters: \\t\", f[item].filters()\n",
    "            print \"ncattrs: \\t\", f[item].ncattrs() \n",
    "            print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing module example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubler(number):\n",
    "    \"\"\"\n",
    "    A doubling function that can be used by a process\n",
    "    \"\"\"\n",
    "    result = number * 2\n",
    "    proc_name = current_process().name\n",
    "    print('{0} doubled to {1} by: {2}'.format(\n",
    "        number, result, proc_name))\n",
    "    return number*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "numbers = [x for x in range(10)] \n",
    "pool = Pool(processes=4)\n",
    "print(pool.map(doubler, numbers))\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset- MODIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The MOD10A1 daily product is gridded in equal area tiles. Each tile consists of a 1200 km by 1200 km data array, which corresponds to 2400 by 2400 cells at 500 m resolution. The following image shows tile locations for MOD10A1 V005 data in a sinusoidal projection.</p>\n",
    "https://modis-land.gsfc.nasa.gov/MODLAND_grid.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sinusoidal_v5.png\" alt=\"Drawing\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read variable by using Multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_read_wrapper(args):\n",
    "    return read_var(*args)\n",
    "\n",
    "def read_var(filename,varname,start=0,end=2400):\n",
    "    proc_name = current_process().name\n",
    "    print('read variable {0} of file {1} by: {2} from {3} to {4}'.format(\n",
    "        varname,filename, proc_name,start,end))\n",
    "    fr = Dataset(filename,'r',format='NETCDF4')\n",
    "    out=fr[varname][:,:,start:end]\n",
    "    fr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata of target files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='/g/data/u39/public/prep/modis-fc/FC.v302.MCD43A4/FC.v302.MCD43A4.h24v05.2000.005.nc'\n",
    "varname='bare_soil'\n",
    "view_prt(filename,varname)\n",
    "filename='/g/data/u39/public/prep/modis-fc/FC.v302.MCD43A4/FC.v302.MCD43A4.h24v05.2001.005.nc'\n",
    "varname='bare_soil'\n",
    "view_prt(filename,varname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list of target file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=[]\n",
    "for year in range(2000,2008):\n",
    "    filename='/g/data/u39/public/prep/modis-fc/FC.v302.MCD43A4/FC.v302.MCD43A4.h24v05.'+str(year)+'.005.nc'\n",
    "    varname='bare_soil'\n",
    "    path.append((filename,varname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use 1 process to read a list of target files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nprocs=1\n",
    "pool = Pool(processes=nprocs)\n",
    "val=pool.map(multi_read_wrapper, path)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use 2 processes to read a list of target files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nprocs=2\n",
    "pool = Pool(processes=nprocs)\n",
    "val=pool.map(multi_read_wrapper, path)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use 4 process to read a list of target files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nprocs=4\n",
    "pool = Pool(processes=nprocs)\n",
    "val=pool.map(multi_read_wrapper, path)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use multiprocessing module to read a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "val=[]\n",
    "paths=[]\n",
    "\n",
    "nprocs=4\n",
    "pool = Pool(processes=nprocs)\n",
    "\n",
    "for year in range(2000,2008):\n",
    "    args=[]\n",
    "    filename='/g/data/u39/public/prep/modis-fc/FC.v302.MCD43A4/FC.v302.MCD43A4.h24v05.'+str(year)+'.005.nc'\n",
    "    varname='bare_soil'\n",
    "    f=Dataset(filename,'r')\n",
    "    dimns=f[varname].shape\n",
    "    for iproc in range(nprocs):\n",
    "        start=iproc*dimns[-1]/nprocs\n",
    "        end=(iproc+1)*dimns[-1]/nprocs\n",
    "        args.append((filename,'bare_soil',start,end))\n",
    "    val=pool.map(multi_read_wrapper, args)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multiple processes accessing a single file (Parallel HDF5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel HDF5 implementation layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/phdf5_layers.png\" alt=\"Drawing\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is typical scheme when working on Raijin with the Lustre file system. On VDI, we simply employed the local storage or NFS which may give rise to different IO performance characteristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python code to parallelly accessing a single file via MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>application: parallel_io.py</p>\n",
    "<nl>\n",
    "<li>argv[1] = 1 (collective) or 0 (independent)</li>\n",
    "<li>argv[2] = file name</li>\n",
    "<li>argv[3] = variable name</li>\n",
    "<li>argv[4] = 0,1,2 the dimension index to be splitted among MPI ranks </li>\n",
    "</nl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "val=[]\n",
    "paths=[]\n",
    "\n",
    "nprocs=' 2'\n",
    "along_dim=' 1'\n",
    "prog_name=\" python parallel_io.py\"\n",
    "collective=' 0'\n",
    "\n",
    "for year in range(2000,2008):\n",
    "    args=[]\n",
    "    filename=' /g/data/u39/public/prep/modis-fc/FC.v302.MCD43A4/FC.v302.MCD43A4.h24v05.'+str(year)+'.005.nc'\n",
    "    varname=' bare_soil'\n",
    "    cmd_line=\"mpirun -np \"+nprocs+prog_name+collective+filename+varname+along_dim\n",
    "    print cmd_line\n",
    "    out=commands.getoutput(cmd_line)\n",
    "    print out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collective vs. independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>MPI has two flavors of operation: collective, which means that all processes have to\n",
    "participate (and in the same order), and independent, which means each process can\n",
    "perform the operation (or not) whenever and in whatever order it pleases.</p>\n",
    "\n",
    "With NetCDF4/HDF5, the modifications to file metadata must be done collectively, such as:\n",
    "<nl>\n",
    "<li>Opening or closing a file</li>\n",
    "<li> Creating or deleting new datasets, groups, attributes, or named types</li>\n",
    "<li> Changing a dataset shape</li>\n",
    "<li> Moving or copying objects in the file</li>\n",
    "</nl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contiguous access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/parallel_access_x.png\" alt=\"Drawing\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-contiguous Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/parallel_access_y.png\" alt=\"Drawing\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collective calls can effectively reduce the number of IO operations for non-contiguous access.\n",
    "Collective calls means all processes of the communicator must participate in calls in the right order.\n",
    "<img src=\"images/collective.png\" alt=\"Drawing\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>application: collective_io.py</p>\n",
    "<nl>\n",
    "<li>argv[1]=col (collective) or idp (independent)</li>\n",
    "<li>argv[2]=x (contiguous access) or y (non-contiguous access)</li></nl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=[]\n",
    "paths=[]\n",
    "\n",
    "nprocs=' 4'\n",
    "prog_name=\" python collective_io.py\"\n",
    "\n",
    "for col_idp in [' col',' idp']:\n",
    "    for along_dim in [' y',' x']:        \n",
    "        cmd_line=\"mpirun -np \"+nprocs+prog_name+col_idp+along_dim\n",
    "        print cmd_line\n",
    "        out=commands.getoutput(cmd_line)\n",
    "        print out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> 'Parallel HDF5',Quincey Koziol, The HDF group, 2015.</li>\n",
    "<li> 'Portable Parallel I/O: Handling large datasets in heterogeneous parallel environments', Michael Stephan, JULICH, 2013.</li>\n",
    "<li> 'Python and HDF5',Andrew Collette, 2014.</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
